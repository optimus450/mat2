1)import numpy as np
import matplotlib.pyplot as plt

# Load the data
data = np.loadtxt('ex2data1.txt', delimiter=',')
X = data[:, 0]  # Exam 1 scores
y = data[:, 1]  # Admission result (0 or 1)

# Add intercept term (bias)
X = np.column_stack([np.ones(X.shape[0]), X])

# Initialize parameters (theta)
theta = np.zeros(2)

# Cost function for linear regression
def cost_function(theta, X, y):
    m = len(y)
    h = X.dot(theta)
    return (1/(2*m)) * np.sum((h - y) ** 2)

# Gradient descent for linear regression
def gradient_descent(X, y, theta, alpha, iterations):
    m = len(y)
    for _ in range(iterations):
        h = X.dot(theta)
        theta -= (alpha/m) * X.T.dot(h - y)
    return theta

# Hyperparameters
alpha = 0.01
iterations = 1500

# Run gradient descent to find the optimal theta
theta_optimal = gradient_descent(X, y, theta, alpha, iterations)

# Print the optimal theta
print(f"Optimal theta: {theta_optimal}")

2)
import numpy as np
import matplotlib.pyplot as plt

# Load data for multivariate regression
data = np.loadtxt('ex2data1.txt', delimiter=',')
X = data[:, :2]  # Exam 1 and Exam 2 scores
y = data[:, 2]  # Admission result (0 or 1)

# Add intercept term (bias)
X = np.column_stack([np.ones(X.shape[0]), X])

# Initialize parameters (theta)
theta = np.zeros(X.shape[1])

# Cost function for multivariate linear regression
def cost_function(theta, X, y):
    m = len(y)
    h = X.dot(theta)
    return (1/(2*m)) * np.sum((h - y) ** 2)

# Gradient descent for multivariate linear regression
def gradient_descent(X, y, theta, alpha, iterations):
    m = len(y)
    for _ in range(iterations):
        h = X.dot(theta)
        theta -= (alpha/m) * X.T.dot(h - y)
    return theta

# Hyperparameters
alpha = 0.01
iterations = 1500

# Run gradient descent to find the optimal theta
theta_optimal = gradient_descent(X, y, theta, alpha, iterations)

# Print the optimal theta
print(f"Optimal theta: {theta_optimal}")

3)
import numpy as np
import matplotlib.pyplot as plt

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Logistic regression cost function
def cost_function_logistic(theta, X, y):
    m = len(y)
    h = sigmoid(X.dot(theta))
    return (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

# Gradient descent for logistic regression
def gradient_descent_logistic(X, y, theta, alpha, iterations):
    m = len(y)
    for _ in range(iterations):
        h = sigmoid(X.dot(theta))
        theta -= (alpha/m) * X.T.dot(h - y)
    return theta

# Generate random dataset for logistic regression
np.random.seed(0)
X = np.random.rand(100, 2) * 100  # Random exam scores
y = (X[:, 0] + X[:, 1] > 100).astype(int)  # Admission result based on score sum

# Add intercept term (bias)
X = np.column_stack([np.ones(X.shape[0]), X])

# Initialize parameters (theta)
theta = np.zeros(X.shape[1])

# Hyperparameters
alpha = 0.01
iterations = 1500

# Run gradient descent to find the optimal theta
theta_optimal = gradient_descent_logistic(X, y, theta, alpha, iterations)

# Print the optimal theta
print(f"Optimal theta for logistic regression: {theta_optimal}")

# Plot decision boundary
def plot_decision_boundary(theta, X, y):
    x_min, x_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    y_min, y_max = X[:, 2].min() - 1, X[:, 2].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    Z = sigmoid(np.c_[np.ones((xx.ravel().shape[0], 1)), xx.ravel(), yy.ravel()].dot(theta))
    Z = Z.reshape(xx.shape)

    plt.contour(xx, yy, Z, levels=[0.5], cmap="RdBu", linestyles="solid")
    plt.scatter(X[:, 1], X[:, 2], c=y, cmap="RdBu", marker='x')
    plt.show()

# Plot decision boundary with optimal theta
plot_decision_boundary(theta_optimal, X, y)
4)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Regularized logistic regression cost function
def cost_function_regularized(theta, X, y, lambda_):
    m = len(y)
    h = sigmoid(X.dot(theta))
    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    reg_cost = (lambda_/(2*m)) * np.sum(theta[1:]**2)
    return cost + reg_cost

# Regularized gradient descent
def gradient_descent_regularized(X, y, theta, alpha, iterations, lambda_):
    m = len(y)
    for _ in range(iterations):
        h = sigmoid(X.dot(theta))
        theta[1:] -= (alpha/m) * (X[:, 1:].T.dot(h - y) + lambda_ * theta[1:])
        theta[0] -= (alpha/m) * np.sum(h - y)
    return theta

# Generate random dataset for logistic regression
np.random.seed(0)
X = np.random.rand(100, 2) * 100  # Random exam scores
y = (X[:, 0] + X[:, 1] > 100).astype(int)  # Admission result based on score sum

# Add intercept term (bias)
X = np.column_stack([np.ones(X.shape[0]), X])

# Map features to polynomial terms (e.g., degree 6)
poly = PolynomialFeatures(degree=6)
X_poly = poly.fit_transform(X)

# Initialize parameters (theta)
theta = np.zeros(X_poly.shape[1])

# Hyperparameters
alpha = 0.01
iterations = 1500

# Experiment with different values of lambda
lambda_values = [0, 1, 100]
for lambda_ in lambda_values:
    theta_optimal = gradient_descent_regularized(X_poly, y, theta, alpha, iterations, lambda_)
    print(f"Optimal theta for lambda={lambda_}: {theta_optimal}")

# Plot decision boundaries for different lambda values
def plot_decision_boundary_regularized(theta, X, y):
    x_min, x_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    y_min, y_max = X[:, 2].min() - 1, X[:, 2].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    Z = sigmoid(np.c_[np.ones((xx.ravel().shape[0], 1)), xx.ravel(), yy.ravel()].dot(theta))
    Z = Z.reshape(xx.shape)

    plt.contour(xx, yy, Z, levels=[0.5], cmap="RdBu", linestyles="solid")
    plt.scatter(X[:, 1], X[:, 2], c=y, cmap="RdBu", marker='x')
    plt.show()

# Plot for one value of lambda (e.g., lambda=1)
plot_decision_boundary_regularized(theta_optimal, X_poly, y)